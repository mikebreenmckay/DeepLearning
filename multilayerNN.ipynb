{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Neural Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Points\n",
    "\n",
    "* Perceptrons never converge on non-linearly separable data\n",
    "* Logistic Regresson will converge but only for two classes\n",
    "* For multiple classes Softmax regression\n",
    "* Softmax regression only creates linear boundaries\n",
    "* Multilayer perceptron can handle multiple classes and complex boundaries but needs more data and tuning\n",
    "* Multilayer perceptron is referred to as a fully-connected feedforward neural network\n",
    "* Multilayer is essentially softmax with some extra hidden layers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some best practices\n",
    "\n",
    "* Start with Logistic Regression and/or Softmax to get a baseline, then try to beat that performance with a more complicated model. \n",
    "* For the output layer we use the softmax activation function and 2 nodes for binary classification\n",
    "* For the hidden layers we could use the sigmoid activation function however it is now more commone to use the ReLU function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ReLU function\n",
    "\n",
    "* Rectified Linear Unit\n",
    "* Piecwise sigma(z) = z if z > 0, else 0, or sigma(z) = max(0,z)\n",
    "* most common\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multilayer perceptron basic arhictecture design considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Hidden Layer must have non-linear activation function\n",
    "* Deep vs. Wide?\n",
    "* Wide and Shallow: ex. 1 hidden layer with many nodes\n",
    "* Narrow and deep: ex. many hidden layers each with a few nodes\n",
    "* Both can technically work but the have pros and cons\n",
    "* Wide and Shallow: Needs lots of hidden units and is prone to memorization (vs learning)\n",
    "* Narrow and Deep: Needs less hidden units, generalized better, however it is harder to train due to vanishing/exploding gradient problems\n",
    "* How do we initialize the weights? \n",
    "    * If we initialize all to 0 then all hidden units have the same value, which makes them redundant\n",
    "    * Initialize to small random numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bulding the multilayer perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.10.8\n",
      "IPython version      : 8.14.0\n",
      "\n",
      "numpy     : 1.24.3\n",
      "pandas    : 2.0.2\n",
      "matplotlib: 3.7.1\n",
      "torch     : 2.0.1\n",
      "\n",
      "conda environment: n/a\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -v -p numpy,pandas,matplotlib,torch --conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
